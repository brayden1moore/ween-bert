{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love you even if you don't\\nYou've got your knife up to my throat\\nWhy do you want to see me bleed?\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 402/402 [03:27<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.songlyrics.com/ween-lyrics/'\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "resp = requests.get(url)\n",
    "soup = bs(resp.text,\"lxml\")\n",
    "songs = soup.find_all('tr')\n",
    "\n",
    "for s in tqdm(songs):\n",
    "    lyr = s.find('a')['href']\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(lyr)\n",
    "        allTxt = resp.text\n",
    "        soup = bs(allTxt,\"lxml\")\n",
    "        verses = soup.find_all('p',{'class':'songLyricsV14 iComment-text'})[0].get_text().replace('\\n\\n\\n','\\n\\n').split('\\n\\n')\n",
    "        for v in verses:\n",
    "            if v != '':\n",
    "                v = v.replace('\\n\\n','\\n')\n",
    "                clear_output(wait=True)\n",
    "                display(v)\n",
    "                with open(f'weenLyricsHOLDING.txt', 'a', encoding=\"utf-8\") as f:\n",
    "                    f.write(f'{v}[split]')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load models\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load and tokenize text\n",
    "with open('weenLyrics.txt','r') as f:\n",
    "    text = f.read().split('[split]')\n",
    "inputs = tokenizer(text, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\n",
    "inputs['labels'] = inputs['input_ids'].detach().clone()\n",
    "\n",
    "# Randomly select 15% of tokens to mask\n",
    "rand = torch.rand(inputs['input_ids'].shape)\n",
    "maskArr = (rand < 0.15) * (inputs['input_ids'] != 101) * (inputs['input_ids'] != 102) * (inputs['input_ids'] != 0)\n",
    "\n",
    "# Get indices of masked tokens\n",
    "maskIndices = []\n",
    "for i in range(maskArr.shape[0]):\n",
    "    maskIndices.append(torch.flatten(maskArr[i].nonzero()).tolist())\n",
    "\n",
    "# Mask the tokens\n",
    "for i in range(maskArr.shape[0]):\n",
    "    inputs['input_ids'][i, maskIndices[i]] = 103\n",
    "\n",
    "# Create dataset class\n",
    "class WeenSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx].detach().clone() for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "dataset = WeenSet(inputs)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 544/544 [01:22<00:00,  6.61it/s, loss=0.297] \n",
      "Epoch 1: 100%|██████████| 544/544 [01:17<00:00,  6.99it/s, loss=0.0928] \n",
      "Epoch 2: 100%|██████████| 544/544 [01:22<00:00,  6.57it/s, loss=0.052]  \n",
      "Epoch 3: 100%|██████████| 544/544 [01:22<00:00,  6.57it/s, loss=0.0487] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt \n",
    "from IPython.display import clear_output\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 4\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        inputIds = batch['input_ids'].to(device)\n",
    "        attentionMask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=inputIds, attention_mask=attentionMask, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "model.save_pretrained(r\"E:\\Models\\bert-finetuned\\weenbert-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "modelPath = r\"E:\\Models\\bert-finetuned\\weenbert-2\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained(modelPath)\n",
    "baseModel = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:34<00:00, 32.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import clear_output, Markdown\n",
    "\n",
    "with open('weenlyrics.txt','r') as f:\n",
    "    text = f.read().split('[split]')\n",
    "\n",
    "text = list(filter(lambda v: 'We do not have the lyrics for' not in v,text))\n",
    "\n",
    "tests = 5000\n",
    "baseCorr = []\n",
    "finetuneCorr = []\n",
    "\n",
    "for i in tqdm(range(tests)):\n",
    "    lyrIdx = round(random.random() * len(text)-1)\n",
    "    verse = text[lyrIdx]\n",
    "    splitText = verse.replace('\\n',' \\n ').replace(',',' , ').replace('.',' . ').replace('\"',' \" ').replace('(',' ( ').replace(')',' ) ').split(' ')[:60]\n",
    "    maskVal = '.'\n",
    "\n",
    "    while maskVal == ',' or maskVal == '.' or maskVal == '\\n' or maskVal == '' or len(maskVal)<4 or len(splitText)<2 or 'We do not have the lyrics' in verse:\n",
    "        lyrIdx = round(random.random() * len(text)-1)\n",
    "        verse = text[lyrIdx]\n",
    "        splitText = verse.replace('\\n',' \\n ').replace(',',' , ').replace('.',' . ').replace(' \" ','\"').replace(' ( ','(').replace(' ) ',')').split(' ')[:60]\n",
    "        maskIdx = round(random.random() * len(splitText)-1)\n",
    "        maskVal = splitText[maskIdx].replace('\"','').replace('.','').replace(',','')\n",
    "\n",
    "    splitText[maskIdx] = '[MASK]'\n",
    "    prompt = ' '.join(splitText).replace(' \\n ','\\n').replace(' , ',',').replace(' . ','.')\n",
    "\n",
    "    encodings = tokenizer(prompt, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\n",
    "    inputIds = encodings['input_ids'].to(device)\n",
    "\n",
    "    try:\n",
    "        maskIdx = (inputIds == 103).flatten().nonzero().item()\n",
    "        attentionMask = encodings['attention_mask'].to(device)\n",
    "\n",
    "        # Base\n",
    "        baseModel.to(device)\n",
    "        outputs = baseModel(input_ids=inputIds, attention_mask=attentionMask)\n",
    "        logits = outputs.logits\n",
    "        soft = logits.softmax(dim=-1)\n",
    "        arg = soft.argmax(dim=-1).view(-1)\n",
    "        baseGuessVal = tokenizer.convert_ids_to_tokens(arg[maskIdx].item())\n",
    "\n",
    "        # Finetuned\n",
    "        model.to(device)\n",
    "        outputs = model(input_ids=inputIds, attention_mask=attentionMask)\n",
    "        logits = outputs.logits\n",
    "        soft = logits.softmax(dim=-1)\n",
    "        arg = soft.argmax(dim=-1).view(-1)\n",
    "        guessVal = tokenizer.convert_ids_to_tokens(arg[maskIdx].item())\n",
    "    except:\n",
    "        guessVal = 'I dunno man...'\n",
    "        baseGuessVal = 'I dunno man...'\n",
    "\n",
    "    baseCorr.append(int(baseGuessVal==maskVal.lower()))\n",
    "    finetuneCorr.append(int(guessVal==maskVal.lower()))\n",
    "\n",
    "    #clear_output(wait=True)\n",
    "    #print(f'Masked Verse: \\n\\n{prompt}\\n')\n",
    "    #print(f'Finetuned Guess: {guessVal}')\n",
    "    #print(f'Base Guess: {baseGuessVal}')\n",
    "    #print(f'Correct Answer: {maskVal}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3012\n",
      "0.4052\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.array(baseCorr).mean())\n",
    "print(np.array(finetuneCorr).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
